{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9LNO84EabcyN8IHPU6AKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hajonghyun/inflearn_ML_from_the_foundation/blob/main/7_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# 선형회귀 이론 및 실습 # 7 Ensemble\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## velog 주소\n",
        "\n",
        "https://velog.io/@changhtun1/Python-Decision-Tree-%EC%9D%B4%EB%A1%A0-%EB%B0%8F-%EC%8B%A4%EC%8A%B5\n",
        "\n",
        "## 유튜브 주소\n",
        "\n",
        "https://youtu.be/OAg7vOFjVck?si=rPLVlwN58ZtPR1J3\n",
        "\n",
        "## 연습 데이터 url\n",
        "\n",
        "https://drive.google.com/drive/folders/149jcCyJFKKG5MFaPNWnYYqM2EkzgRz2P?usp=sharing\n",
        "\n",
        "\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "If3beJ-w8YvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "- 앙상블 모형의 좋은 성능을 내기 위해서는 다양한 종류의 오차를 만들어야 하고, 그러기 위해서는 다양한 알고리즘을 사용해야 한다\n",
        "\n",
        "# 🏰 다양한 오차를 만들기 위한 방법\n",
        " 1. 모델을 다양하게 ensemble\n",
        " 2. 훈련 데이터를 다양하게 사용\n",
        "    - 훈련 세트의 서브셋을 무작위로 구성하여 모델을 학습시키기\n",
        "        - 배깅: 훈련 세트의 중복을 허용하여 샘플링을 하는 방식(통계학에선 bootstrapping이라고 불림)\n",
        "        - 페이스팅: 훈련 세트의 중복을 허용하지 않고 샘플링 하는 방식\n",
        "        - 전반적으로 배깅이 더 나은 성능. 교차검증이 젤 좋음.\n",
        " 3. 훈련 데이터의 feature를 다양하게 사용\n",
        " ---\n",
        " ---"
      ],
      "metadata": {
        "id": "qHj3_7lhZN8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅1. 모델을 다양하게 ensemble"
      ],
      "metadata": {
        "id": "B71ldDl41AKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNUe-nOWE8Sa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드\n",
        "iris = load_iris()\n",
        "x = iris.data[:,2:] # 꽃잎의 길이, 너비만\n",
        "y = iris.target\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,shuffle=True,random_state=2021) # shuffle을 하지 않으면 y 00011111 앞부분만 잘림.\n",
        "\n",
        "# 약한 학습기 구축\n",
        "log_model = LogisticRegression()\n",
        "rand_model = RandomForestClassifier()\n",
        "svm_model = SVC()\n",
        "\n",
        "# 앙상블 모델 구축\n",
        "voting_model = VotingClassifier(\n",
        "    estimators = [('lr',log_model),\n",
        "                  ('rf',rand_model),\n",
        "                  ('svc',svm_model),],\n",
        "    voting = 'hard' #직접투표방법\n",
        ")\n",
        "\n",
        "# 모델 비교\n",
        "for model in (log_model,rand_model,svm_model,voting_model):\n",
        "    model.fit(x_train,y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "    print(model.__class__.__name__+\"정확도: \"+str(accuracy_score(y_test,y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTz8NQE6QB_x",
        "outputId": "599fab6e-75f5-46f6-b570-eb11124aa7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression정확도: 1.0\n",
            "RandomForestClassifier정확도: 0.9555555555555556\n",
            "SVC정확도: 1.0\n",
            "VotingClassifier정확도: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅2. 훈련 데이터를 다양하게 사용"
      ],
      "metadata": {
        "id": "ySCufS7z1inm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# 모델 구축\n",
        "bag_model = BaggingClassifier(\n",
        "    DecisionTreeClassifier(),\n",
        "    n_estimators=500, # 약한 학습기(결정 트리) 500개 생성\n",
        "    max_samples= 0.05, # 0.0~1.0 사이 실수 선택(실수*샘플 수) 혹은 샘플 수 지정\n",
        "    bootstrap=True, # True:배깅 False: 페이스팅\n",
        "    n_jobs=-1 # 사용할 CPU 코어 수\n",
        ")\n",
        "\n",
        "# 모델 학습\n",
        "bag_model.fit(x_train,y_train)\n",
        "\n",
        "# 모델 예측\n",
        "y_pred = bag_model.predict(x_test)\n",
        "\n",
        "# 모델 평가\n",
        "print(bag_model.__class__.__name__,accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sGcl4fIcpUr",
        "outputId": "88d95184-60d8-41f7-a563-a724bc482ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaggingClassifier 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅3. 훈련 데이터의 feature를 다양하게 사용\n",
        "- 위의 2번에서는 데이터셋의 row를 변형했다면, 3번에서는 데이터셋의 column을 변형하여 예측기에 대한 오차 다양성을 제공.\n",
        "\n",
        "- 랜덤 패치 방식: 훈련 특성과 샘플을 모두 샘플링\n",
        "- 랜덤 서브스페이스 방식: 훈련 샘플은 모두 사용하고, 특성만 샘플링"
      ],
      "metadata": {
        "id": "e0dkqesD1pkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩랜덤포레스트\n",
        "\n",
        "- 랜덤포레스트는 일반적으로 배깅방법을 사용한 결정트리 앙상블 모델이다.\n",
        "- 그래서 BaggingClassifier에 DecisionTreeClassifier를 넣는 대신, RandomForestClassifier를 사용할 수 있다.\n",
        "- 그래서 RandomForestClassifier는 DecisionTreeClassifier와 BaggingClassifier 매개변수 모두 가지고 있다.\n",
        "- 랜덤포레스트 모델은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 것이 아니라, 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을 채택하여 무작위성을 더 가지게 된다.\n",
        "- 이를 통해 약간의 편향은 손해보지만,(= 편향이 증가 = 그래프 단순 = 과소적합 위험) 더욱 다양한 트리를 만들므로 분산을 전체적으로 낮추어서(= 민감도 낮춤 = 과적합 위험 줄임) 더 훌륭한 모델을 만들 수 있다.\n",
        "(분산: 데이터가 평균 주위에 흩어져 있는 정도, 분산이 크면 과적합 위험이 있다.)\n",
        "\n"
      ],
      "metadata": {
        "id": "UFZcD_2E54oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_model = RandomForestClassifier(\n",
        "    n_estimators = 500, # 예측가 500개\n",
        "    max_leaf_nodes = 16, # 자식 노드의 최대 개수\n",
        "    n_jobs=-1 # cpu 코어 사용 개수\n",
        ")\n",
        "\n",
        "rnd_model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = rnd_model.predict(x_test)\n",
        "\n",
        "print(\"RandomForest 모델: \", accuracy_score(y_pred,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7N9d_jW18IQ",
        "outputId": "f144cf2a-d54f-4445-9740-4b3a5b6cf710"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest 모델:  0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 특성 중요도 => RF 블랙박스로 알아내기 가능.\n",
        "\n",
        "- 랜덤포레스트는 성능이 좋다는 장점말고, 특성의 상대적 중요도를 측정하기 쉽다.(트리기반 모델은 특성 중요도 제공)\n",
        "\n",
        "- 사이킷런에서는 어떤 특성을 사용한 노드가 평균적으로 불순도를 감소시키는지 확인하여 특성 중요도를 측정하고, 훈련이 끝나고 난 뒤에 특성마다 자동으로 점수를 계산하고 저장한다.\n",
        "- 저장된 값은 **featureimportances 변수**에 저장되어 있다.\n",
        "\n",
        "### **주의해야할 점**\n",
        "내가 만든 모델에 대해서만 적용되는 특성 중요도인 것이지, 전체적인 것에 모두 적용되는 것이 아니다!!!"
      ],
      "metadata": {
        "id": "uUI6HGZW9VHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특성의 상대적 중요도 => featuresimportances 변수\n",
        "iris = load_iris()\n",
        "x = iris.data[:,:]\n",
        "y = iris.target\n",
        "\n",
        "rnd_model = RandomForestClassifier(\n",
        "    n_estimators = 500,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rnd_model.fit(x,y)\n",
        "\n",
        "for feature_name,feature_impo in zip(iris['feature_names'],rnd_model.feature_importances_):\n",
        "    print(feature_name,' : ',feature_impo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nryVPKxZ8jYO",
        "outputId": "9aa35651-0cb9-4286-dd31-d2ba980ee414"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm)  :  0.09777817211823935\n",
            "sepal width (cm)  :  0.022718811029106997\n",
            "petal length (cm)  :  0.43857573562709223\n",
            "petal width (cm)  :  0.4409272812255614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚩엑스트라 트리\n",
        "랜덤포레스트보다 편향이 더 심하고, 분산은 더욱 낮춘 모델. 과적합 위험 감소.\n",
        "- RandomForestClassifier와 ExtraTreesClassifier 중 어떤 것이 더 좋을지는 판단하기 어렵기 때문에, 교차검증을 통해서 서로 비교해보고, 더 나은 모델을 선택하여 그리드 탐색방법을 사용해 하이퍼파라미터 튜닝을 한다."
      ],
      "metadata": {
        "id": "utvHpWNX68xW"
      }
    }
  ]
}